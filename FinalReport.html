<!DOCTYPE html>

<html>
<head>
  <title>README</title>

	<meta charset="utf-8"/>
  <style type="text/css">
html {
    /* the line height value of 1.9 is fixed for a line height multiple of 1.4 in the text view */
    font: normal 106.2%/1.9 serif;
}

body {
    font-family: '.SF NS Text', Helvetica, Georgia, serif;
    word-wrap: break-word;
    margin: 0;
    padding: 0;
    border: 0;
    vertical-align: baseline;
    overflow:hidden;
    background-color: #f2f2f2;
    color: #3c3c3c;
}

/**
 *
    Wrappers
 *
 */

#wrapper {
    position: fixed;
    width: 99.9%;
    height: 100%;
    overflow-y: scroll;
    overflow-x: hidden;
    font-size: 1.0em;
}

#wrapper::-webkit-scrollbar {
    width: 8px;
    height: 10px;
    -webkit-transition: all .45s ease-in;
    position: relative;
}
#wrapper::-webkit-scrollbar-button:start:decrement, #wrapper::-webkit-scrollbar-button:end:increment {
    height: 0px;
    display: block;
    background-color: transparent;
}
#wrapper::-webkit-scrollbar-track-piece {
    background-color:
    transparent;
    -webkit-border-radius: 6px;
}
#wrapper::-webkit-scrollbar-thumb:vertical {
    height: 50px;
    background-color: #c8c8c8;
    -webkit-border-radius: 6px;
    -webkit-transition: all .45s ease-in;
}

#content {
    width: 850px;
    margin: 0 auto 0 auto;
    padding: 30px 0 30px 0;
}

/**
 *
    Headings
 *
 */

h1, h2, h3, h4, h5, h6 {
    text-rendering: optimizeLegibility;
    line-height: 1;
    margin: 0.5rem 0;
}

h1, h2, h3, h4, h5, h6 {
    text-rendering: optimizeLegibility;
    line-height: 1;
    margin: 0.5rem 0;
}

h1, 
h2, 
h3 { margin-bottom: 1rem; }

h1 { font-size: 2.75rem; }
h2 { font-size: 2.25rem; }
h3 { font-size: 1.75rem; }
h4 { font-size: 1.25rem; }
h5 { font-size: 1.00rem; }
h6 { font-size: 0.85rem; }

/**
 *
	Paragraphs
 *
 */

p {
	margin: 0 0 1.5em;
}

/**
 *
    Block quotes
 *
 */

blockquote {
    margin: 0 0 1.5em;
    width: 96%;
    padding: 0 10px;
    border-left: 3px solid #ddd;
    color: #777;
}

/**
 *
	Code Blocks
 *
 */

pre code {
	word-wrap: normal;
	white-space: pre-wrap;
    border: none;
    padding: 0;
    background-color: transparent;
    -webkit-border-radius: 0;
}

pre {
	white-space: pre-wrap;
    width: 96%;
    margin-bottom: 24px;
    overflow: hidden;
    padding: 3px 10px;
    -webkit-border-radius: 3px;
    background-color: #eee;
    border: 1px solid #ddd;
}

code {
	white-space: nowrap;
	font-family: monospace;
    padding: 2px;
    -webkit-border-radius: 3px;
    background-color: #eee;
    border: 1px solid #ddd;
}

small {
	font-size: 65%;
}

/**
 *
    Dictionary Definition
 *
 */

dt {
	display: inline;
    font-weight:bold;
}

dd {
	display: block;
}

/**
 *
    Anchors
 *
 */

a {
    color: #308bd8;
    text-decoration:none;
}
a:hover {
    text-decoration: underline;
}

/**
 *
    Horizontal rules
 *
 */

hr {
    width: 100%;
    margin: 3em auto;
    border: 0;
    color: #eee;
    background-color: #ccc;
    height: 1px;
    -webkit-box-shadow:0px 1px 0px rgba(255, 255, 255, 0.75);
}

/**
 *
    Lists
 *
 */

ol, ul {
    list-style-position: outside;
    padding-left: 0;
    margin-left: 1.5em;
}

ol li, ul li {
    text-align: -webkit-match-parent;
}

li p {
    margin: 0.5em 0 0.5em;
}

ol ol, ol ul, ul ul, ul ol {
    padding-left: 1.5em;
}

/**
 *
    Tables
 *
 */

table {
    margin-left: auto;
    margin-right: auto;
    margin-bottom: 24px;
    border-bottom: 1px solid #ddd;
    border-right: 1px solid #ddd;
    border-spacing: 0;
}

table th {
    padding: 3px 10px;
    background-color: #eee;
    border-top: 1px solid #ddd;
    border-left: 1px solid #ddd;
}

table tr {
}

table td {
    padding: 3px 10px;
    border-top: 1px solid #ddd;
    border-left: 1px solid #ddd;
}

/**
 *
    Images
 *
 */

img {
    border: none;
    display: block;
    margin: 1em auto;
    max-width: 100%;
}

/**
 *
    Marks
 *
 */

mark {
    background: #fefec0;
    padding:1px 3px;
}

/**
 *
    Footnotes
 *
 */
.footnote {
    font-size: 0.8rem;
    vertical-align: super;
}
.footnotes ol {
    font-weight: bold;
}
.footnotes ol li {
    text-align: -webkit-match-parent;
}

.footnotes ol li p {
    font-weight: normal;
}

/**
 *
    Figures and Captions
 *
 */
caption {
    font-size: 1.2rem;
    font-weight: bold;
    margin-bottom: 5px;
}

figure {
    display: block;
    text-align: center;
}

figcaption {
    font-size: 0.8rem;
    font-style: italic;
}

/**
 *
    Custom classes
 *
 */

.shadow {
    -webkit-box-shadow: 0 2px 4px #999;
}

.source {
    text-align: center;
    font-size: 0.8em;
    color: #777;
    margin: -40px auto;
}
#wrapper {
    width: 99.9%;
}
#content {
    width: 850px;
    margin-left: auto;
    margin-right: auto;
}

/* Mobile support */
@media only screen and (max-device-width:1024px) {
    html {
        overflow: auto;
    }

    #wrapper {
        overflow: auto;
        position: relative;
    }
}/* html elements */

body {
    background-color: #1a1a1a;
    color: #bebebe;
}
a {
    color: #308bd8;
}
hr {
    color: #666;
    background-color: #666;
    -webkit-box-shadow: none;
}
pre {
    background-color: #222;
    border-color: #3c3c3c;
}
code {
    background-color: #222;
    border-color: #3c3c3c;
}
blockquote {
    border-color: #333;
    color: #999;
}
table {
    border-color: #3c3c3c;
}
table th {
    background-color: #222;
    border-color: #3c3c3c;
}
table td {
    border-color: #3c3c3c;
}
mark {
    background: #bc990b;
    color:#000;
}

/* unique elements */

#wrapper::-webkit-scrollbar-thumb:vertical {
    background-color: #525252;
}
#top-fader {
    background: -webkit-gradient(linear, left top, left bottom, from(rgba(26,26,26,1)), to(rgba(26,26,26,0)));
}
#bottom-fader {
    background: -webkit-gradient(linear, left bottom, left top, from(rgba(26,26,26,1)), to(rgba(26,26,26,0)));
}

/* custom formatting classes */

.shadow {
    -webkit-box-shadow: 0 2px 4px #000;
}@media print {
/* Printing support.
 * Override all printing colors to match the Light theme.
 */
img, pre, blockquote, table, figure {
    page-break-inside: avoid;
}
body {
    background-color:#fff;
}
#wrapper {
    position: static;
    overflow: hidden;
    color: #000;
    width: 100%;
    margin: 0 auto;
}
.footnotes {
    page-break-before: always;
}
#content {
    margin: 0 auto;
    padding: 0;
    width: 98%;
}
#top-fader, #bottom-fader {
    display: none;
}
hr {
    color:#ddd;
    background-color:#ddd;
    -webkit-box-shadow:0px 1px 0px #ddd;
}
pre {
    background-color:transparent;
    border: 1px solid #ddd;
}
code {
    background-color:transparent;
    border: 1px solid #ddd;
}
blockquote {
    border-left: 3px solid #ddd;
    color: #000;
}
ol {
    /*
     * Override the list style when printing to ensure list
     * markers won't get cut.
     */
    list-style-position: inside;
    padding-left: 0;
    margin-left: 0;
}
table {
    border-bottom: 1px solid #ddd;
    border-right: 1px solid #ddd;
}
table th {
    background-color:transparent;
    border-top: 1px solid #ddd;
    border-left: 1px solid #ddd;
}
table td {
    border-top: 1px solid #ddd;
    border-left: 1px solid #ddd;
}
mark {
    background:transparent;
    color: #000;
}
.source {
    color: #000;
}
}
  </style>
</head>
<body>
  <div id="wrapper">
    <div id="content">
<h1>Terrain Mapping Applications Using Drone Image Data</h1>

<p><strong>Henry Mound</strong></p>

<h2>Introduction</h2>

<p>This independent study was, in a sense, a continuation of the research I started last summer with Professor Grant regarding the computer vision applications of drones. The original goal was to calculate image depth in real-time using Tello EDU drones.</p>

<h2>Stereo Vision</h2>

<p>The original Tello drones, which I used over the summer, do not support drone swarm applications by default. I was previously able to control two Tello drones simultaneously by using two different WiFi antennas and instructing each antenna to connect to each Tello individually. With the updated Tello EDU, however, swarm connectivity is supported out of the box.</p>

<p>In order to control multiple EDUs in swarm, each drone must know which base station network to connect to and the password of said network. I was able to achieve this using a TP-Link router and sending the following command: <code>ap TP-Link_0F46 10534212</code>. The ap command tells the drone to enter station mode, and the following two parameters are the network name and password, respectively. After each drone was connected to the network, the next challenge was to find the IP addresses of each Tello EDU connected. For this, I entered into the local router configuration page and found the starting addresses of all connected devices. In this case, the starting IP address was <code>192.168.10.1</code>. I put together a simply Python script that pinged 500 addresses starting at 192.168.10.1 and incrementing by 1. With the Tellos connected, the IP addresses 192.168.10.101 and 192.168.10.102 were the only ones with a reply. I could then further identify which address corresponded to which drone by using packet sender to issue basic flight commands. </p>

<figure>
<img src="images/packet_sender.png" alt="Packet Sender" />
<figcaption>Packet Sender</figcaption>
</figure>

<p>Continuing with the goal of implementing stereo vision, the next step was to pull video from both drones. It was easy to connect to both drones simultaneously using Python and issue basic flight commands, but pulling video proved more problematic. After many failed attempts and consulting both community-developed and official Tello APIs, I discovered that the firmware of the Tello EDUs does not support the <code>streamon</code> command, which initializes a video feed, while the drones are in station mode. This was a big problem for the goal of the independent study. I tried connecting to the drones outside of station mode with external WiFi antennas to no avail. I also tried running two separate virtual machines simultaneously, each with independent WiFi cards, and connecting each VM to a Tello. This, still, did not allow for simultaneous video streams. I reached this block at the middle of the third week of the semester. After consulting with Professor Grant, we decided to pivot to a new direction.</p>

<h2>SFM: Structure From Motion</h2>

<p>On the subject of image processing and drone footage and after discussing different computer vision algorithms with Professor Scharstein, I started exploring COLMAP. COLMAP is an end-to-end image-based 3D reconstruction pipeline. There are open-source versions for both the GUI-based and command-line programs. As this independent study focused heavily around the use of drones, I found it important to take advantage of the image applications that are unique to only drone applications. Along the topic of depth reconstruction, I set to work using COLMAP to generate 3D models of BiHall. With the smaller form factor of the Tello and its default 30m hight limit, it would have been a difficult drone to use for this application. So, using a Mavic Pro I had access to, I took sequential images from a constant height looping around all of BiHall. For the fist run, I took a total of 76 images of the building from equally-spaced angles roughly 5 degrees apart. Using the latest pre-release version of COLMAP and the pictures of BiHall, I was able to generate a point-cloud 3D model of the building. Here are some of the input images and their output:</p>

<figure>
<img src="images_compressed/colmap.png" alt="COLMAP" />
<figcaption>COLMAP</figcaption>
</figure>

<p>This process was very graphics-heavy. Because the software is open-source and supported on all major operating systems, I was able to try it with various setups. The point-cloud result was encouraging, but I was hoping to actually generate a 3D model with the mapped building textures. According to the documentation, this is something the COLMAP software supports. I ran the algorithm on macOS, but discovered that the graphical limitations did not allow for full 3D modeling. I also tested in a Windows environment, but without a graphics card that supported Kuda, this was not possible. A similar open-source program, Regard3D, yielded similar results: </p>

<figure>
<img src="images/regard3d.png" alt="Regard 3D" />
<figcaption>Regard 3D</figcaption>
</figure>

<p>The potential applications of the point-cloud results were highly encouraging. The ability to accurately model the shape of a building using just drone pictures has countless applications. One such application that was particularly exciting to me was generating high-quality three-dimensional maps of landscapes using only images and/or video frames. I wanted to focus on creating reconstructions and extrapolating data using only image files because, not requiring other data allows for more versatile input. Though even consumer drones like the Mavic Pro can record GPS and local positioning data, using such output would potentially introduce many complications and would require data that may not be guaranteed when extrapolating to new applications. In the spirit of only using images and video as input data, and after consulting with Professor Grant, we decided to move forward with studying visual odometry. Visual odometry refers to the process of determining the position and orientation of a camera using just input images. </p>

<h2>Visual Odometry</h2>

<p>Using <a href="https://github.com/uoip/monoVO-python">a public repository on GitHub</a>, I started exploring path mapping with drones. This specific visual odometry implementation uses OpenCV and Lucas-Kanade optical flow to produce image movement results. I spent a significant amount of time implementing LK optical flow and feature tracking with OpenCV during my <a href="https://github.com/henrymound/2018-Summer-Research">summer research</a>, so this was a great next step. The public repository requires a text file input of the &#8216;actual&#8217; video path as well as the input images to generate a comparison between the two. Because I did not have access to points that mapped an actual path, I modified the program to not require this input. The next step was to calculate the camera projection matrix for the Tello I was using to take the input images. With the help of Professor Grant and a computer vision calibration board, we found the (fx, fy) focal lengths and (cx, cy) principal points of the Tello EDU camera. These four points, along with the width and height resolution parameters were used as inputs in the visual odometry program to compensate for the potential camera distortion. To collect input data, I wrote a program to fly the Tello in a straight line at a constant velocity for 50 meters and save each frame to a folder. For this application, it was necessary to fly the Tello outside. It was not possible to gather enough image data from an indoor flight. Due to wind interference, amongst other things, it was difficult to gather images of a clean flight path. Even holding the drone by hand and walking in a straight line did not yield a completely straight flight path. After more than half a dozen attempts under various conditions, unavoidable interference in the images resulted in grainy visual odometry results. The best attempt using the Tello to map a straight line started off well for the first 10 meters or so:</p>

<figure>
<img src="images_compressed/monovo1.png" alt="monovo1" />
<figcaption>monovo1</figcaption>
</figure>

<p>However, small disruptions in the input images quickly caused an inaccurate result. The final computed path of a straight-line video was highly inaccurate:</p>

<figure>
<img src="images_compressed/monovo2.png" alt="monovo2" />
<figcaption>monovo2</figcaption>
</figure>

<p>Even with the Mavic, small errors in the image disparity calculations early on skewed the final result greatly. For example, when flying in a rough circle with the Mavic, the odometry mapping started off quite well. Below is an image with the actual path overlaid on the path predicted using the modified visual odometry program: </p>

<figure>
<img src="images/mavic_interum.png" alt="Mavic Monovo Interim" />
<figcaption>Mavic Monovo Interim</figcaption>
</figure>

<p>At approximately halfway though the flight, a problematic series of frames throws the algorithm off:</p>

<figure>
<img src="images/mavic_final.png" alt="Mavic Monovo Final" />
<figcaption>Mavic Monovo Final</figcaption>
</figure>

<p>At this stage, around week eight of the independent study process, we moved to a new area of study. The visual odometry results were problematic, but promising. My results from working with odometry suggested that, with error-correcting modifications to the algorithm and more easily computable input frames, it would be possible to generate accurate flight maps. But, why would such flight mapping data be useful? Because visual odometry processes the flight frames sequentially to create its output map, each input frame&#8217;s relative position can be calculated and stored. This has many beneficial applications. One such application is image stitching.</p>

<h2>Image Stitching</h2>

<p>Often times, image stitching algorithms use input images and apply feature tracking algorithms to calculate their relative position and create a final composite image. By using visual odometry to calculate each the relative position of various frames in a flight, I believed it may be possible to implement a more accurate image stitching algorithm for drone applications. To start my study of image stitching, I turned back to the Tello for image data collection. Again, I used Python and OpenCV. OpenCV has libraries for implementing various types of image stitching. Two such image stitching options are scanning and panorama stitching. Scan image stitching takes input images such that the input images lay on the same two-dimensional plane and can be stitched together without applying warping transformations (only rotational and scalar transformations). I started my dive into image stitching with OpenCV by focusing on scan image stitching. </p>

<h3>Scan Image Stitching</h3>

<p>I started by writing a program that connected to the Tello EDU and saved video frames into an array at determined strafing intervals. The program then stitched the images saved in the array and stored the result. The results were less than ideal. To investigate why the stitching was performing so poorly, I examined the interim stitching calculations and how, as images were added to the array, the resulting stitched image changed. Below are the results: </p>

<figure>
<img src="images/tello_scan/stitched3.png" alt="Tello Scans Stitch 1" />
<figcaption>Tello Scans Stitch 1</figcaption>
</figure>

<figure>
<img src="images/tello_scan/stitched5.png" alt="Tello Scans Stitch 2" />
<figcaption>Tello Scans Stitch 2</figcaption>
</figure>

<figure>
<img src="images/tello_scan/stitched7.png" alt="Tello Scans Stitch 3" />
<figcaption>Tello Scans Stitch 3</figcaption>
</figure>

<figure>
<img src="images/tello_scan/stitched9.png" alt="Tello Scans Stitch Final" />
<figcaption>Tello Scans Stitch Final</figcaption>
</figure>

<p>By looking at these results, it occurred to me that there may not have been enough change in the input images along the x axis. I tried the same approach, but by taking images as the Tello moved vertically upwards. This still did not yield good results. I predicted that, because more image disparity was needed, aerial shots of the ground would be a more appropriate input source for the scan method of image stitching. So, using the Mavic&#8217;s adjustable camera, I flew at a constant 80m altitude taking various images in a line with the camera positioned exactly downwards. Using OpenCV&#8217;s Python scan stitching library, I wrote a program to stitch the resulting Mavic images together. These were a few of the total 17 input images and resulting composite: </p>

<figure>
<img src="images/mavic_scan/1.JPG" alt="Mavic Scans Input 1" />
<figcaption>Mavic Scans Input 1</figcaption>
</figure>

<figure>
<img src="images/mavic_scan/2.JPG" alt="Mavic Scans Input 2" />
<figcaption>Mavic Scans Input 2</figcaption>
</figure>

<figure>
<img src="images/mavic_scan/3.JPG" alt="Mavic Scans Input 3" />
<figcaption>Mavic Scans Input 3</figcaption>
</figure>

<figure>
<img src="images/mavic_scan/4.JPG" alt="Mavic Scans Input 4" />
<figcaption>Mavic Scans Input 4</figcaption>
</figure>

<figure>
<img src="images/mavic_scan/5.JPG" alt="Mavic Scans Input 5" />
<figcaption>Mavic Scans Input 5</figcaption>
</figure>

<figure>
<img src="images/mavic_scan/RESULTS.JPG" alt="Mavic Scans Result" />
<figcaption>Mavic Scans Result</figcaption>
</figure>

<p>The above results were highly encouraging. The stitching quality and accuracy was very high for the initial run. With the prospect of using visual odometry path calculations in combination with scans stitching in mind, I set out to try and test the feasibility of stitching together a high-resolution campus-wide aerial image. To start, I chose an area of campus to take aerial pictures of using the Mavic. I took a total of 478 (4000 x 3000) images of a square region of campus bound by four landmarks: the edge of Ross Lang, College Street, Hmkl Way, and Chateau Rd. The captured images totaled of 2.74 Gigabytes and covered the entirety of the region outlined below in red:</p>

<figure>
<img src="images/mavic_mapping/area_captured.png" alt="Mavic Area Surveyed" />
<figcaption>Mavic Area Surveyed</figcaption>
</figure>

<p>Though I was skeptical of my computer&#8217;s ability to simply take in all of the captured images to create a large stitched result, my first approach was to read in all 478 images and try and stitch them together. After many hours of 100% CPU usage, 100% of my computer&#8217;s total 8GB of RAM, and over 30GB of swap space usage, this approach failed due to an underlying array error. I concluded that the input images were too large and overwhelming to the algorithm. To counteract this, I decided to stitch images together in batches with the intention of then stitching together the batch results. I experimented with different input batch sizes. I started by simply stitching all input images in sets of 2. This yielded only minor changes, as the distance between the input images was small. A batch size of 30 images was very processor intensive and sometimes failed. So, I scaled down the images within the stitching program using OpenCV to lighten the processing load. Even with downscaling, several of the batch results using 30 input images failed, like in the case below: </p>

<p><img src="images_compressed/failed30downscaled.png" alt="Mavic Failed 30 Batch" />
<a href="images/mavic_batches/failed30downscaled.JPG">Full Resolution</a></p>

<p>There were also instances with just minor stitch flaws:</p>

<p><img src="images_compressed/minorfail30batch.png" alt="Mavic Failed 30 Batch" />
<a href="images/mavic_batches/minorfail30batch.JPG">Full Resolution</a></p>

<p>As well as instances where the result was quite successful:</p>

<p><img src="images_compressed/successful30batch.png" alt="Mavic Failed 30 Batch" />
<a href="images/mavic_batches/successful30batch.JPG">Full Resolution</a></p>

<p>I tried using only every other image in the flight data set as input to reduce the input size. This approach worked for most of the stitches, but failed in specific cases. After trying many different combinations of batch size and image scaling (and chewing up a lot of storage space), I found what seemed to be the sweet spot: downscaling the input images to 30% of their original size and stitching together batches of 10 images. Upon trying to stitch the resulting batch images, however, I ran into issued like the one below:</p>

<p><img src="images_compressed/failednestedbatch.jpg" alt="Mavic Failed Nested Batch" />
<a href="images/mavic_batches/failednestedbatch.JPG">Full Resolution</a></p>

<p>At this stage in the process, I believe nested batching proved problematic because, even to a person, the input images did not have a clear way of fitting together. In other words, two sequential images did not fit together in a line. For example, here are two sequentially processed sub-batch stitch results:</p>

<p><img src="images_compressed/problematic_sub_stitching.jpg" alt="Mavic Failed Nested Batch" />
<a href="images/mavic_batches/problematic_sub_stitching.JPG">Mavic Failed Nested Batch</a></p>

<p>Though the continuity issue demonstrated by the above image could likely have been remedied by further cherry-picking input images, the original goal of this endeavor was to created a final stitched image with little human oversight. So, I modified my approach slightly. The original flight path consisted of 10 attempted straight-line aerial paths. I grouped these 10 vertical strafes together and removed the few images taken between the distinct paths. Each of these paths consisted of approximately 50 images. I set to work on a &#8216;smart_batch&#8217; program that, instead of using a set batch size, smart batch would take in several input parameters to create an ideal batch size. Then, my goal was to stitch all these completed sub batches into one final very tall high resolution image. I would repeat this process for all 10 grouped paths. I wrote the smart batch program to take several parameters: lower-bound batch size (minimum number of images used per batch), upper-bound batch size, aspect ratio goal (the goal of the resulting stitched batch image as a multiple of the height of one of the input images), scale factor (used to scale the input images upon read), and a &#8216;no_shrink&#8217; boolean variable that determines whether the resulting stitched batch image is allowed to get smaller as more constituent images are added. After testing several combinations of input parameters, I found what seemed to be a sweet spot: an aspect ratio goal of 1.6, no_shrink enabled, a minimum batch size of 3, and a maximum batch size of 20. Below is a stitched image using these parameters. The <a href="images/path5.JPG">full resolution of this image is (4911 × 18309)</a>:</p>

<figure>
<img src="images_compressed/path5.jpg" alt="Smart Stitch Results" />
<figcaption>Smart Stitch Results</figcaption>
</figure>

<p>Unfortunately, not all 10 paths stitched as well as the one above. Upon closer examination of the source data and the step-by-step results of combining the constituent batch-stitched images, in several cases the pictures captured did not lay accurately enough in a line to be used for successful stitching. Given more time, more processing power, and access to greater storage capacity, I would like to continue this work at a greater scale. The overall goal would be to stitch a final lossless image of the entire campus using image data captured by a preprogrammed, autonomous flight path. </p>

<h3>Panorama Image Stitching</h3>

<p>It was at this stage that the semester started to come to a close. My last endeavor was to implement panorama stitching. Once again, I started with the Tello. The programming was quite straight-forward and involved flying the Tello to a specified height and capturing frames at equally-spaced rotation intervals, ending once the drone completed a full rotation. Below illustrates the stitching process and resulting image:</p>

<figure>
<img src="images/tello_pano_progress.png" alt="Tello Pano Steps" />
<figcaption>Tello Pano Steps</figcaption>
</figure>

<p>And another example of a completed pano stitch using the Tello:</p>

<figure>
<img src="images/tello_pano2.png" alt="Tello Pano 2" />
<figcaption>Tello Pano 2</figcaption>
</figure>

<p>I repeated this same process with the Mavic at a higher altitude:</p>

<p><img src="images_compressed/djipano1.png" alt="Mavic Pano 1" />
<a href="images/djipano1.JPG">Full Resolution</a></p>

<p><img src="images_compressed/djipano2.png" alt="Mavic Pano 2" />
<a href="images/djipano2.JPG">Full Resolution</a></p>

<p>These results were encouraging and had far fewer complications that the scan stitch experiments. However, instead of taking in input images, I wanted to be able to use only a video file as input for panorama stitching (as well as scans stitching in the future). So, I captured a short 40 second video of a full drone rotation with the Mavic. I used one of the tools that has proved invaluable this semester: ffmpeg. ffmpeg allows for easy command-line batch image resizing and reformatting. Furthermore, it allowed me to easily break up an input video into frames with a defined fps value. Using ffmpeg, I broke the Mavic&#8217;s pano video into full resolution constituent images at one second intervals. After running the 40 extracted frames through a modified stitching program, I got the following results:</p>

<p><img src="images_compressed/djipanovideo.JPG" alt="Mavic Pano Video" />
<a href="images/djipanovideo.JPG">Full Resolution</a></p>

<hr />

<h2>Future Work</h2>

<p>The work completed in this independent study paves the way for several further avenues of research. </p>

<h4>Stereo Vision</h4>

<p>Though it was not possible to pull simultaneous video streams from two Tello EDU drones in station mode, it is possible there are workarounds. A server could act as a base point and be configured to accept several streams, appearing to the drones as separate computers. Another option would be to use a different type of drone. Though the drone stereo vision implementation may be problematic due to the difficulty of enforcing such strict camera alignment, it would still be a very interesting avenue to explore.</p>

<h4>Structure From Motion</h4>

<p>The results achieved using COLMAP and Regard3D show the very real possibility of mapping terrain and buildings using drones. Though mapping calculations are computationally expensive, further avenues of research include: generating point clouds in real time, analyzing and splitting recorded drone video to isolate specific areas of interest before mapping, and creating 3D renders using both point clouds and image texture information. </p>

<h4>Visual Odometry</h4>

<p>Visual odometry proved problematic during the course of this study. In many instances, a few problematic frames of video skewed the final generated path. One further avenue of study would be to analyze all frames in the data set and remove any frame(s) that differ greatly from ones previous before using the data set as input to visual odometry. This would potentially mitigate the mapping problems we encountered. Furthermore, it would be interesting to try and optimize the odometry algorithm for input images taken from a bird&#8217;s-eye-view drone camera. The resulting data could be used to improve and optimize image stitching algorithms.</p>

<h4>Image Stitching</h4>

<p>Panoramic image stitching proved quite straight-forward during the course of this study. Scan imaging stitching, however, had its problems. It would be interesting to continue the work of smart image batching and to implement new functionality to make the process smarter and faster. I believe more promising results could be achieved by outsourcing the computation required to a dedicated computer. In the case of Middlebury, this could be achieved by using time on Gattaca or outsourcing to unused computers on campus. A campus-wide stitched image would be incredibly large and difficult to manipulate/view as a single file. By creating a dedicated application, it may be possible to design an interface to explore and pan across the full-resolution stitched image without having to load and process the whole file at once. </p>
    </div>
  </div> <!-- End wrapper -->
</body>
</html>