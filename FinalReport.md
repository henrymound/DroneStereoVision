# Final Report
	This independent study was, in a sense, a continuation of the research I started last summer with Professor Grant regarding the computer vision applications of drones. The original goal was to calculate image depth in real-time using Tello EDU drones. The original Tello drones, which I used over the summer, do not support drone swarm applications by default. I was previously able to control two Tello drones simultaneously by using two different WiFi antennas and instructing each antenna to connect to each Tello individually. With the updated Tello EDU, however, swarm connectivity is supported out of the box.
	In order to control multiple EDUs in swarm, each drone must know which base station network to connect to and the password of said network. I was able to achieve this using a TP-Link router and sending the following command: ``ap TP-Link_0F46 10534212``. The ap command tells the drone to enter station mode, and the following two parameters are the network name and password, respectively. After each drone was connected to the network, the next challenge was to find the IP addresses of each Tello EDU connected. For this, I entered into the local router configuration page and found the starting addresses of all connected devices. In this case, the starting IP address was ``192.168.10.1``. I put together a simply Python script that pings 500 addresses starting at 192.168.10.1 and incrementing by 1. With the Tellos connected, the IP addresses 192.168.10.101 and 192.168.10.102 were the only ones with a reply. I could then futher identify which address corresponded to which drone by using packet sender to issue basic flight commands. 
![Packet Sender](images/packet_sender.png) 
	Continuing with the goal of implementing stereo vision, the next step was to pull video from both drones. It was easy to connect to both drones simultaneously using Python and issue basic flight commands, but pulling video proved more problematic. After many failed attempts and consulting both community-developed and official Tello APIs, I discovered that the firmware of the Tello EDUs does not support the ``streamon`` command, which initializes a video feed, while the drones are in station mode. This was a big problem for the goal of the independent study. I tried again connecting to the drones outside of station mode with external WiFi antennas to no avail. I also tried running two seperate virtual machines simultaneously, each with independent WiFi cards, and connecting each VM to a Tello. This, still, did not allow for simultaneous video streams. I reached this block at the middle of the third week of the semester. As workarounds for simultaneous video had not worked for me over the summer, I was not optomistic about the prospect of them working for the Tello EDUs when not in station mode. So, after consulting with Professor Grant, we decided to pivot to a new direction.
	On the subject of image processing and drone footage and after discussing different computer vision algorithms with Professor Scharstein, I started exploring COLMAP. COLMAP is an end-to-end image-based 3D reconstruction pipeline. There are open-source versions for both the GUI-based and command-line programs. As this independent study focused heavily around the use of drones, I found it important to take advantage of the image applications that are unique to only drone applications. Along the topic of depth reconstruction, I set to work using COLMAP to generate 3D models of buildings on campus. With the smaller form factor of the Tello and its default 30m hight limit, this would have been difficult. So, I using a Mavic Pro I had access to, I took sequential images from a constant height looping around all of BiHall. For the fist run, I took a total of 76 images of the building from equally-spaced angles roughly 5 degrees apart. Using the latest pre-release version of COLMAP and the pictures of BiHall, I was able to generate a point-cloud 3D model of the building. Here are some of the input images and their output:
	![COLMAP](images/colmap.png)
	This process was very graphics-heavy. Because the software is open-source and supported on all major operating systems, I was able to try it with various setups. The point-cloud result was encouraging, but I was hoping to actually generate a 3D model with the mapped building textures. According to the documentation, this is something the COLMAP software supports. I ran the algorithm on macOS, but discovered that the graphical limitations did not allow for full 3D modeling. I also tested in a Windows environment, but without a graphics card that supported Kuda, this was not possible. 
	The potential applications of the point-cloud result were highly encouraging. The ability to accuratly model the shape of a building using just drone pictures has countless applications. One such application that was particularly exciting to me was generating high-quality three-dimentional maps of landscapes using just images and video frames. I wanted to focus on creating reconstructions and extrapolating data using only image files because not requiring other data allows for more versatile input. Though even consumer drones like the Mavic Pro can record GPS and local positioning data, using such output would potentially introduce many complications and would require data that may not be guaranteed when extrapolating to new applications. In this spirit of only using images and video as input data, and after consulting with Professor Grant, we decided to move forward with studying visual odometry. Visual odometry refers to the process of determining the position and orientation of a camera using just input images. 
	Using [a public repository on GitHub](https://github.com/uoip/monoVO-python), I started exploring path mapping with drones. This specific visual odometry implementation uses OpenCV and Lucas-Kanade optical flow to produce image movement applications. I spent a significant amount of time implementing LK optical flow and feature tracking with OpenCV during my summer research, so this was a great next step. The public repository requires a text file input of the 'actual' video path as well as the input images to generate a comparison between the two. Because I did not have access to points that map an actual path, I modified the program to not require this input. The next step was to calculate the camera projection matrix for the Tello I was using to take the input images. With the help of Professor Grant and a computer vision calibration board, we found the fx, fy focal lengths and cx, cy principal points of the Tello EDU camera. These four points, along with the width and height resolution parameters were used as inputs in the visual odometry program to compensate for the potential camera distortion. To collect input data, I wrote a program to fly the Tello in a straight line at a contant velocity for 50 meters and save each frame to a folder. For this application, it was necissary to fly the Tello outside to enough image data to cover a large space. Due to wind interference, amongst other things, it was difficult to gather images of a clean flight path. Even holding the drone by hand and walking in a straight line did not allow for a completely straight flight path. After more than half a dozen attempts under various conditions, unavoidable interference in the images resulted in grainy visual odometry results. The best attempt using the Tello to map a straight line started off well for the first 10 meters or so:
	![monovo1](images/monovo1.png)
However, small disruptions in the input images quickly caused an inaccurate result. The final computed path of a straight-line video was highly innacurate:
	![monovo2](images/monovo2.png)
	Even with the Mavic, small errors in the image disparity calculations early on would skew the final result greatly. For example, when flying in a rough circle with the Mavic, the odometry mapping started off quite well. Below is an image with the actual path overlayed on the path predicted using the modified visual odometry program: 
	![Mavic Monovo Interum](images/mavic_interum.png) 
	At approximatly halfway though the flight, a problematic series of frames throws the algorithm off:
	![Mavic Monovo Interum](images/mavic_final.png) 
	At this stage, around week eight of the independent study process, moved to a new area of study. The visual odometry results were problematic but promising. My results from working with odometry suggest that with error-correcting modifications to the algorithm and more easily computable input frames, accurate flight maps are possible to generate. But, why would such flight mapping data be useful? Because visual odometry processes the flight frames sequentially to create its output map, each input frame's relative position can be calculated and stored. This has many beneficial applications. One such application is image stitching.
	Often times, image stitching algorithms use input images any apply feature tracking algorithms to calculate their relative position and create a final composite image. By using visual odometry to calculate each the relative position of various frames in a flight, I believed it may be possible to implement a more accurate image stitching algorithm for drone applications. To start my study of image stitching, I turned back to the Tello for image data collection. Again, I used Python and OpenCV. OpenCV has libraries for implementing various types of image stitching. Two such image stitching options are scanning and panorama stitching. Scan image stitching takes input images such that the input images are from the same two-dimensional plane and can be stitched together without applying warping transformations and only rotational and scalar transformations. I started my dive into image stitching with OpenCV by focusing on scan image stitching. 
	I started by writing a program that connects to the Tello EDU and saves video frames into an array at determined intervals as the Tello strafes to the side in one direction. The program then stitches the images saved in the array and outputs the result. The results were less than ideal. To investigate why the stitching was performing so poorly, I examined the interup stitching calculations and how, as images were added to the array, the resulting stitched image changed. Below are the results: 
	![Tello Scans Stitch 1](images/tello_scan/stitched3.png) 
	![Tello Scans Stitch 2](images/tello_scan/stitched5.png) 
	![Tello Scans Stitch 3](images/tello_scan/stitched7.png) 
	![Tello Scans Stitch Final](images/tello_scan/stitched9.png) 
	By looking at these results, it occured to me that there was not enough change in the input images along the x axis. I tried the same approach but by taking images as the Tello moved vertically upwards. This still did not yield good results. I predicted that, becaue more image disparity was needed, aerial shots of the ground would be a more appropriate input source for the scan method of image stitching. So, using the Mavic's adjustable camera, I flew at a constant altitude taking various images in a line with the camera positioned perfectly downwards. Using OpenCV's Python scan stitching library, I wrote a program to stitch the resulting Mavic images together. These were a few of the total 17 input images and resulting composite: 
	![Mavic Scans Input 1](images/mavic_scan/1.JPG) 
	![Mavic Scans Input 2](images/mavic_scan/2.JPG) 
	![Mavic Scans Input 3](images/mavic_scan/3.JPG) 
	![Mavic Scans Input 4](images/mavic_scan/4.JPG) 
	![Mavic Scans Input 5](images/mavic_scan/5JPG) 
	![Mavic Scans Result](images/mavic_scan/RESULT.JPG) 
	The above results were highly encouraging. The stitching quality and accuracy was very high for the initial run. With the prospect of visual odometry path calculations in combination with scans stitching in mind, I set out to try and test the feasability of stitching together a high-resolution campus-wide aerial image. To start, I chose an area of campus to take aerial pictures of using the Mavic. I took a total of 478 (4000 x 3000) images of a square region of campus bound by four landmarks: the edge of Ross Lang, College Street, Hmkl Way, and Chateau Rd. The captured images totaled of 2.74 Gigabytes and covered the entirety of the region outlined below in red:
	![Mavic Area Surveyed](images/mavic_mapping/)
	
# Citations 
- https://demuc.de/papers/schoenberger2016sfm.pdf

# Topics Covered During the Process
1. Stereo Vision
	- Sending byte commands to the Tellos to put them into station mode (including SSID and password of local station).
	- Using Python to send commands to both drones at the same time. This was a different approach to the one I took over the summer, during which I connected each drone to a different antenna. With this new approach, however, I can control more than two Tellos at the same time - giving the swarm capability a lot more power.
	- Unfortunatly, in station mode, the Tellos do not accept the 'streamon' command that allows video data to be transfered.
	- Tried creating multiple virtual machines and using each VM to connect to a Tello and recieve video data that way. This proved impossible.
2. COLMAP 
	- Using drone photos from the Mavic around BiHall, I was able to generate a point cloud that was representative of the building. This was really encouraging. However, the algorithm to generate the results was too sophisticated for me to change/work with within this independent study process.
	- ![COLMAP Results](images/colmap.png)
3. monoVO
	- Next, with the guidance of Professor Grant, I started diving into visual odometry and its applications. Modifying the repository on GitHub, I started feeding in flight images from the Tello. Part of this process involved camera calibration. 
4. FFMPEG


# Essay Structure
1. Introduction
2. Process/Work Done
3. Reflection
	1. Further work
	2. Difficulties/roadblocks